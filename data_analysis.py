# -*- coding: utf-8 -*-
"""data_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PrrV390GlDeyPsKRrahrFHdMeWC2aFhO

# 게임의 인기도 분석 프로젝트

## 1. 문제 정의
* Task : Steam 플랫폼에서 게임의 인기여부 예측
 * 다양한 지표를 잡을 수 있으나, 여기선 긍정적 평가 비율이 높을수록 인기있다고 판단.
* Performance : 인기 / 비인기의 형태로 이진 분류
 * (긍정 / 전체평가)에 대한 비율을 매겨서 이를 지표로 삼음
* Experience : steamspy 사이트를 크롤링한 자료.
 * 해당 사이트에서 데이터를 제공하긴 하나, 자료형태로 제공하진 않으므로 자체적인 크롤링 과정이 필수.

## 2. 관련 데이터 수집

### 2-1. Data 크롤링
* 업로드 된 파일 중 data_collector 파일 참고.
* steamspy 사이트에 요청을하여서 1000개 게임에 대한 정보를 뽑아내는 코드. (기본값 : [0])
* request_appid의 인자로 [0, 1, 2]라는 값을 넣으면 총 3000개의 정보를 뽑아낼 수 있음
* data_collector 파일을 실행시키고 어느정도의 시간이 흐르면 steam_data.csv와 data_info.txt 파일이 생성.
 * data_info.txt - 해당 데이터를 크롤링 한 시점 저장
 * steam_data.csv - 사용 할 데이터 저장
 > 단, 데이터를 크롤링만 한 다음 특별한 정제 없이 자료화 하였으므로, 정제과정이 필요함!

### 2-2. Data check
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 현재 분석 할 자료의 경우 사전에 크롤링하여서 구글드라이브에 저장해놓았음. (원본 데이터셋 참고)
!gdown --id "18W8JeGs5TkfUfy3g4kkSW9t4Vscfab6o"  # csv 파일
!gdown --id "10LUmuubAeErb1u8KJS7TO0tIye7KsWQq"  # txt 파일

steam_data = pd.read_csv("steam_data.csv", encoding='ISO-8859-1')
infile = open("date_info.txt", "r")
crawl_date = infile.readlines()
infile.close()

# 데이터 크롤링 시점 확인.
print(crawl_date)

# 들고온 데이터 dataframe 형태로 확인.  (**미정제)
steam_data.head()

steam_data.iloc[104]

"""* 데이터 설명
 * appid - 각 게임의 steam 플랫폼 상에서 고유 식별번호.
 * name - 게임 이름
 * developer - 개발자 / 개발회사
 * publisher - 배포업체
 * score rank - * 특정 경우에 대한 평점, 불필요정보.
 * positive - 유저들의 긍정적 평가 수
 * negative - 유저들의 부정적 평가 수
 * userscore - * 특정 경우에 대한 유저평점, 불필요정보.
 * owners - 현재 게임을 보유한 사람의 수 범위  (정제 필요.)
 * average forever - 게임 플레이 시간 평균, 전 기간 기준
 * average 2weeks - 게임 플레이 시간 평균, 최근 2주 기준
 * median forever - 게임 플레이 시간 중간값, 전 기간 기준
 * median 2weeks - 게임 플레이 시간 중간값, 최근 2주 기준
 * price - 게임의 원가, 999 => $9.99 로 해석.
 * initial price - 할인을 고려한 판매 가격
 * discount - 할인 가격
 * ccu - 동시접속자수의 고(高)점 (기준은 크롤링 전날 data)
 * languages - 지원 언어  (정제 필요.)
 * genre - 게임 장르 (정제 필요.)
 * tags - 게임에 매겨진 태그, (정제 필요.)
"""

steam_data.shape

"""## 3. Data 정제

### 3-1. 완결성 파악

#### 3-1-1. 결측치 파악
"""

# score_rank, userscore의 경우 col 삭제.
steam_data = steam_data.drop(["score_rank", "userscore"], axis=1)

steam_data.isnull().sum()
# developer, publisher, genre의 경우 빠진 부분은 채워넣기.

steam_data.head()

df_nan_cnt = steam_data.isnull().sum(1)
df_nan_index = df_nan_cnt[(df_nan_cnt > 0)]

steam_data.loc[df_nan_index.index]  # 결측치가 있는 데이터만 표현

"""#### 3-1-2. 결측치 보완"""

# developer나 publisher의 정보가 없는 경우 이 두개의 정보를 동일하게 갱신.
steam_data[["developer", "publisher"]] = steam_data[["developer", "publisher"]].fillna(method='ffill', axis=1)
steam_data[["developer", "publisher"]] = steam_data[["developer", "publisher"]].fillna(method='bfill', axis=1)

steam_data[["developer", "publisher"]].loc[df_nan_index.index]

# 1731 data의 경우 두개 모두 없어서 수기 기입.
pd.set_option('mode.chained_assignment', None) # warning 메세지 강제 무시용. 
steam_data['developer'][2417] = "spleen"
steam_data['publisher'][2417] = "spleen"
pd.set_option('mode.chained_assignment', 'warn')  # 다시 원래대로

steam_data.isnull().sum()

# genre, tag의 경우 수기로 직접 정보를 개입.
print(steam_data['genre'].loc[df_nan_index.index])

pd.set_option('mode.chained_assignment', None) # warning 메세지 강제 무시용. 
# 정보 기입. (편의상 바로 dataframe에 수정하였음.)
steam_data['languages'][1400] = "English, French, German"
steam_data['genre'][166] = "Adventure, Indie"
steam_data['genre'][264] = "Survival, Horror"
steam_data['genre'][469] = "Platform, Action, Adventure"
steam_data['genre'][545] = "Platform, Action, Adventure"
steam_data['genre'][556] = "Platform, Action, Adventure"
steam_data['genre'][746] = "Utilities"
steam_data['genre'][905] = "Action, Adventure"
steam_data['genre'][907] = "Action, Adventure"
steam_data['genre'][917] = "RPG"
steam_data['genre'][918] = "Action"
steam_data['genre'][954] = "Action, Adventure"
steam_data['genre'][959] = "Action, Adventure"
steam_data['genre'][988] = "Utilities"
steam_data['genre'][1023] = "Action, Casual, Indie, Simulation"
steam_data['genre'][1027] = "RPG, Strategy"
steam_data['genre'][1028] = "RPG"
steam_data['genre'][1573] = "Utilities"
steam_data['genre'][1580] = "Adventure, Indie, Strategy"
steam_data['genre'][1788] = "Action, Adventure, Indie, RPG"
steam_data['genre'][1826] = "Game engine"
steam_data['genre'][2504] = "Action"
steam_data['genre'][2637] = "Action"
steam_data['genre'][2646] = "Adventure, Simulation, Sports"
steam_data['genre'][2743] = "RPG, Adventure"
steam_data['genre'][2991] = "Action"
steam_data['tags'][221] = "{'Action':441, 'Zombies':424, 'Co-op':403, 'Third-Person Shooter':316, 'Horror':288, 'Co-Op Campaign':275, 'Online Co-Op':260, 'Adventure':235, 'Third Person':177, 'Survival Horror':170, 'Multiplayer':165, 'Shooter':163, 'Singleplayer':150, 'Local Co-Op':102, 'Female Protagonist':102, 'Survival':94, 'Quick-Time Events':90, 'Gore':88, 'Atmospheric':51, '3D Vision':48}"

pd.set_option('mode.chained_assignment', 'warn')  # 다시 원래대로
print(steam_data['genre'].loc[df_nan_index.index])

steam_data.isnull().sum()  # 일단 NaN 값이 없는 Data 완성.

"""### 3-2. 데이터 확인 및 정제"""

from collections import Counter

steam_data.info()
# object로 구성 된 column을 다른 형태로 분해해야 한다.

"""#### 3-2-1. y data 생성

##### 3-2-1-1. 표본 이상치 제거
"""

full_rate = steam_data['positive'] + steam_data['negative']

full_rate.describe()

# 평가가 1000명 미만인 경우는 데이터 삭제.
full_rate.loc[full_rate < 1000].shape

# 평가가 1000명 미만인 게임의 index.
full_rate.loc[full_rate < 1000]

steam_data = steam_data.drop(full_rate.loc[full_rate < 1000].index, axis=0)

steam_data.shape

"""##### 3-2-1-2. y 데이터 구축"""

pos_rate = steam_data['positive']*100 / (steam_data['positive'] + steam_data['negative'])
pos_rate.isnull().sum()

pos_rate = pos_rate.astype(int)
pos_rate  # 정수 형태로 바꾼 긍정 평가 지수.

pos_rate.describe()

plt.boxplot(pos_rate, flierprops=dict(marker='D', markersize=4, markerfacecolor='r'))
plt.title("positive rate (%)")
plt.show()

outlier_indices = []

Q1 = np.percentile(pos_rate,25) 
Q3 = np.percentile(pos_rate,75)
IQR = Q3 - Q1
outlier_step = IQR * 1.5
outlier_list_col = pos_rate[(pos_rate < Q1 - outlier_step) | (pos_rate > Q3 + outlier_step)].index #filter
outlier_list_col

pos_rate = pos_rate.drop(outlier_list_col)
steam_data = steam_data.drop(outlier_list_col)
print(pos_rate.shape, steam_data.shape)   # y의 이상치 제거

# 긍정 평가가 85% 이상인 게임들이 대중들에게 인기가 있다고 가정. 값 치환.
popular = pd.Series([ 1 if val >= 85 else 0 for val in pos_rate ], index=pos_rate.index)
popular.value_counts()

# pos, neg 데이터는 필요 없으므로 삭제.
steam_data = steam_data.drop(["positive", "negative"], axis=1)
steam_data.info()

"""#### 3-2-2. 문자 데이터 처리

##### 3-2-2-1. owners 정보를 class별로 치환
"""

Counter(steam_data['owners'])  # 정보 확인

owners_label = {"0 .. 20,000" : 0,
                "20,000 .. 50,000" : 0,
                "50,000 .. 100,000" : 0,
                "100,000 .. 200,000" : 1,
                "200,000 .. 500,000" : 1,
                "500,000 .. 1,000,000" : 1,
                "1,000,000 .. 2,000,000" : 2,
                "2,000,000 .. 5,000,000" : 2,
                "5,000,000 .. 10,000,000" : 2,
                "10,000,000 .. 20,000,000" : 3,
                "20,000,000 .. 50,000,000" : 3,
                "50,000,000 .. 100,000,000" : 3,
                "100,000,000 .. 200,000,000" : 4,
                "200,000,000 .. 500,000,000" : 4,
                "500,000,000 .. 1,000,000,000" : 4
                }
print(owners_label)
# class 0 ) 0 ~ 100,000
# class 1 ) 100,000 ~ 1,000,000
# class 2 ) 1,000,000 ~ 10,000,000
# class 3 ) 10,000,000 ~ 100,000,000
# class 4 ) 100,000,000 ~ 1,000,000,000

steam_data = steam_data.replace({'owners' : owners_label})
steam_data = steam_data.rename(columns={'owners' : 'owner_class'})

steam_data['owner_class'].value_counts().sort_index()

"""##### 3-2-2-2. languages 정보에서 지원 언어 개수를 추출"""

print(steam_data['languages'][0])  # 정보 확인
print(steam_data['languages'][2643])

# 지원 언어 개수의 경우 해당 string에서 , 개수 +1개를 하면 된다고 추측 가능.
print( steam_data['languages'][0].count(',')+1 )
print( steam_data['languages'][2643].count(',')+1 )

steam_data['language_count'] = steam_data['languages'].apply(lambda x: x.count(',')+1)

plt.bar(steam_data['language_count'].value_counts().index, steam_data['language_count'].value_counts())
plt.show()

# 추가 check, 단일 언어 중에서 높은 비율만 확인.
steam_data['languages'][steam_data['language_count']==1].value_counts()

"""##### 3-2-2-3. genre 및 tags 정보를 분해"""

genre_tag = set()
genre_list = []

# 우선 genre index에 대해.
for val in steam_data['genre']:
  one_split = val.replace("&", "n").lower().split(", ")
  genre_list.append(one_split)
  for tag in one_split:
    genre_tag.add(tag)
print(genre_tag)  # 전체 장르 태그
print(genre_list[30:40])  # 특정 태그

from ast import literal_eval  # 문자열을 딕셔너리로 변환
index_cnt = 0
for val in steam_data['tags']:
  one_split = list(literal_eval(val.replace("&", "n").lower()).keys())
  genre_list[index_cnt] = genre_list[index_cnt] + one_split
  for tag in one_split:
    genre_tag.add(tag)
  index_cnt += 1
print(genre_tag)  # 전체 장르 태그
print(genre_list[30:40])  # 특정 태그

# 전체 장르 태그만 재정리.
cnt = 1
for v in sorted(list(genre_tag)):
  print('"'+v+'"', end=',    ')
  if cnt%8 == 0: print()
  cnt += 1

df_isdata = pd.DataFrame(index=pos_rate.index)  # 특정 태그로 분류되어 있는지 여부 판단. 1은 True, 0는 False를 의미.
for tag in genre_tag:
  df_isdata["is_"+tag.replace(" ", "_")] = [1 if tag in lists else 0 for lists in genre_list]

df_isdata_check = pd.DataFrame(df_isdata.sum(), columns=['counts'])

df_isdata_check['rate'] = df_isdata_check['counts'] / (popular.count())
min_per = 0.05 # 전체 게임의 min_per*100 % 비중 이하 태그
df_isdata_check.loc[df_isdata_check['rate'] <= min_per]  # 희소 데이터만 확인.

# 희소 태그에 대해선 삭제.
df_isdata = df_isdata.drop(list(df_isdata_check[df_isdata_check['rate'] <= min_per].index), axis=1)
df_isdata

steam_data = steam_data.join(df_isdata)
steam_data.head()

"""##### 3-2-3. 이상치 처리 및 불필요한 행 제거"""

sample = steam_data.drop(["appid", "name", "developer", "publisher", "genre", "tags", "languages"], axis=1)
sample.info()

# outlier 찾아내기
def detect_outliers(df,features):
  outlier_indices = []
  for c in features:
    Q1 = np.percentile(df[c],25)
    Q3 = np.percentile(df[c],75)
    IQR = Q3 - Q1 
    outlier_step = IQR * 1.5 
    outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index #filter
    outlier_indices.extend(outlier_list_col)
  outlier_indices = Counter(outlier_indices)
  multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2) 
  return multiple_outliers

sample.loc[detect_outliers(sample,["average_forever", "average_2weeks", "median_forever", "median_2weeks", "price", "initialprice", "ccu"])]
# 조금 이상이 있다 판단되는 값을 뽑아내기.

# 이상치 삭제 (col 지우기)
detect = detect_outliers(sample, ["average_forever", "average_2weeks", "median_forever", "median_2weeks", "price", "initialprice", "ccu"])
detect[0: 10]

sample = sample.drop( detect )
popular = popular.drop( detect )

"""## 4. 학습을 위한 Data 분리"""

# 사용 할 데이터
print(sample.shape)
print(popular.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(sample, popular)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

"""## 5. Algorithm 이용 훈련"""

from sklearn.svm import SVC  # 분류문제
from sklearn.tree import DecisionTreeClassifier  # 결정트리 분류
from sklearn.tree import export_graphviz
import graphviz
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier  # RF 분류
from sklearn.model_selection import cross_val_score, cross_validate

"""### 5-1. SVM"""

svm_model = SVC(kernel='rbf', C=200000, probability=True)
svm_model.fit(X_train, y_train)
print(f"학습 : {svm_model.score(X_train, y_train)}")
print(f"예측 : {svm_model.score(X_test, y_test)}")

svm_predict = svm_model.predict(X_test)
from sklearn.metrics import (accuracy_score, 
                            confusion_matrix,
                            precision_score, recall_score,
                            f1_score,
                            roc_curve, roc_auc_score)

# 1. 정확도
print(f"정확도 : {accuracy_score(y_test, svm_predict)}")

# 2. 오차 행렬
conf_matrix = confusion_matrix(y_true = y_test, y_pred = svm_predict) 
print(f"오차행렬 :\n{conf_matrix}")  ##  TN FP / FN TP

# 3. 정밀도와 재현율
print(f"정밀도 : {precision_score(y_test, svm_predict)}")
print(f"재현율 : {recall_score(y_test, svm_predict)}")

# 4. F1-score
print(f"F1-score : {f1_score(y_test, svm_predict)}")

# 5. ROC/AUC  
print(f"ROC score : {roc_auc_score(y_test, svm_predict)}")


# 그래프 =====================
import matplotlib.pyplot as plt

# 분류가 1인 친구들 모으기
pred_proba_class1 = svm_model.predict_proba(X_test)[:, 1]
fprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1)

# 시각화 시작
plt.plot(fprs, tprs, label='ROC')
plt.plot([0, 1], [0, 1], label = 'random')
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.legend()

"""### 5-2. 결정트리"""

decision_tree_model = DecisionTreeClassifier(max_depth=6)
decision_tree_model.fit(X_train, y_train)
decision_cross_val = cross_validate(estimator=decision_tree_model, X=X_train, y=y_train, cv=5)
print(f"예측 점수 : {decision_cross_val['test_score'].mean()}")

dot_data = export_graphviz(decision_tree=decision_tree_model,
                           feature_names=X_train.keys(),
                           filled=True,
                           rounded=True,
                           class_names=["nonpopular", "popular"]
                           )
graph = graphviz.Source(dot_data)
graph

# 간추린 형태
plt.figure(figsize=(300,30))
from sklearn import tree
tree.plot_tree(decision_tree_model)

"""### 5-3. Random Forest"""

rf_model = RandomForestClassifier()
cross_val = cross_validate(estimator=rf_model, X=X_train, y=y_train, cv=5)
print(f"average test score :", cross_val['test_score'].mean())

rf_model = ExtraTreesClassifier()
cross_val = cross_validate(estimator=rf_model, X=X_train, y=y_train, cv=5)
print(f"average test score :", cross_val['test_score'].mean())

"""### 5-4. 신경망 이용 학습"""

import numpy as np
import tensorflow as tf

print(X_train.shape, y_train.shape)

layer_model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=64, activation='sigmoid', input_shape=(98,)),
    tf.keras.layers.Dense(units=16, activation='sigmoid'),
    tf.keras.layers.Dense(units=4, activation='sigmoid'),
    tf.keras.layers.Dense(units=1, activation='sigmoid')
])

layer_model.compile(loss='binary_crossentropy')  # 이진 분류
layer_model.summary()

history = layer_model.fit(X_train, y_train, epochs=500)

layer_model.evaluate(X_test, y_test)

arr = layer_model.predict(X_test)
trans = [0 if val<0.5 else 1 for val in arr]
correct = (trans == y_test)
data = correct.value_counts()
true_cnt = data[1]
false_cnt = data[0]
print(f"적중률 : {true_cnt/(true_cnt+false_cnt)}")
print(data)